{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching revisions for article: 2016_United_States_presidential_election\n",
      "Rate limited. Retrying after 1 seconds...\n",
      "Rate limited. Retrying after 1 seconds...\n",
      "Rate limited. Retrying after 1 seconds...\n",
      "Rate limited. Retrying after 1 seconds...\n",
      "Rate limited. Retrying after 1 seconds...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to fetch revision count for 2016_United_States_presidential_election after 5 retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 119\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching revisions for article: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Fetch revision count\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m revs \u001b[38;5;241m=\u001b[39m \u001b[43mget_revision_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(revs)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Fetch revisions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 68\u001b[0m, in \u001b[0;36mget_revision_count\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching revision count for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Retrying...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m attempt)  \u001b[38;5;66;03m# Exponential backoff\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch revision count for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m retries.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Failed to fetch revision count for 2016_United_States_presidential_election after 5 retries."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "def get_article_revisions(title, older_than=None):\n",
    "    \"\"\"\n",
    "    Fetches revisions for a given article.\n",
    "    \"\"\"\n",
    "    url = f'https://api.wikimedia.org/core/v1/wikipedia/en/page/{title}/history'\n",
    "    parameters = {}\n",
    "    if older_than:\n",
    "        parameters['older_than'] = older_than\n",
    "    headers = {\n",
    "        'User-Agent': 'WikiSandbox/ManagingBigData/d.galati@student.utwente.nl'  # Replace with your project and email\n",
    "    }\n",
    "    retries = 5\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=headers, params=parameters)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get(\"revisions\", [])\n",
    "        elif response.status_code == 429:  # Rate-limiting error\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", 1))\n",
    "            print(f\"Rate limited. Retrying after {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "        else:\n",
    "            print(f\"Error fetching revisions for {title}: {response.status_code}. Retrying...\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    raise Exception(f\"Failed to fetch revisions for {title} after {retries} retries.\")\n",
    "\n",
    "def loop_through_revisions(title, from_date=None, olderThanId=None):\n",
    "    \"\"\"\n",
    "    Loops through revisions for an article until a specific timestamp is reached.\n",
    "    \"\"\"\n",
    "    revisions = []\n",
    "    while True:\n",
    "        new_revisions = get_article_revisions(title, older_than=olderThanId)\n",
    "        if not new_revisions:\n",
    "            break\n",
    "        revisions.extend(new_revisions)\n",
    "        if new_revisions[-1][\"timestamp\"] <= from_date:\n",
    "            break\n",
    "        olderThanId = new_revisions[-1][\"id\"]\n",
    "    print(f\"Gathered {len(revisions)} revisions for article: {title}\")\n",
    "    return revisions\n",
    "\n",
    "def get_revision_count(title):\n",
    "    \"\"\"\n",
    "    Fetches the total revision count for an article.\n",
    "    \"\"\"\n",
    "    url = f'https://api.wikimedia.org/core/v1/wikipedia/en/page/{title}/history/counts/edits'\n",
    "    headers = {\n",
    "        'User-Agent': 'WikiSandbox/ManagingBigData'  \n",
    "    }\n",
    "    retries = 5\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        elif response.status_code == 429:  # Rate-limiting error\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", 1))\n",
    "            print(f\"Rate limited. Retrying after {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "        else:\n",
    "            print(f\"Error fetching revision count for {title}: {response.status_code}. Retrying...\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    raise Exception(f\"Failed to fetch revision count for {title} after {retries} retries.\")\n",
    "\n",
    "def save_revisions_to_hdfs(revisions, filename):\n",
    "    \"\"\"\n",
    "    Saves the revisions in an HDFS-compatible format (JSON Lines).\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
    "        for revision in revisions:\n",
    "            json.dump(revision, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")  # Each revision is a separate JSON line\n",
    "\n",
    "def get_us_election_titles():\n",
    "    \"\"\"\n",
    "    Returns lists of Wikipedia article titles for US elections in 2016, 2020, and 2024.\n",
    "    \"\"\"\n",
    "    elections_2016 = [\n",
    "        \"2016_United_States_presidential_election\",\n",
    "        \"2016_Democratic_Party_presidential_primaries\",\n",
    "        \"2016_Republican_Party_presidential_primaries\"\n",
    "    ]\n",
    "    \n",
    "    elections_2020 = [\n",
    "        \"2020_United_States_presidential_election\",\n",
    "        \"2020_Democratic_Party_presidential_primaries\",\n",
    "        \"2020_Republican_Party_presidential_primaries\"\n",
    "    ]\n",
    "    elections_2024 = [\n",
    "        \"2024_United_States_presidential_election\",\n",
    "        \"2024_Democratic_Party_presidential_primaries\",\n",
    "        \"2024_Republican_Party_presidential_primaries\"\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"2016\": elections_2016,\n",
    "        \"2020\": elections_2020,\n",
    "        \"2024\": elections_2024\n",
    "    }\n",
    "\n",
    "# revision from jan first 2016 \n",
    "revisions_from= \"2016-01-01T00:00:00Z\"\n",
    "output_dir = \"election_revisions\"\n",
    "\n",
    "election_titles = get_us_election_titles()\n",
    "\n",
    "for year in [\"2016\", \"2020\", \"2024\"]:\n",
    "    for title in election_titles[year]:\n",
    "\n",
    "        print(f\"Fetching revisions for article: {title}\")  \n",
    "\n",
    "        # Fetch revision count\n",
    "        revs = get_revision_count(title)\n",
    "        print(revs)\n",
    "\n",
    "        # Fetch revisions\n",
    "        print(f\"Fetching article revisions... after {revisions_from}\")\n",
    "        revisions = loop_through_revisions(title, from_date=revisions_from)\n",
    "        \n",
    "        # Save revisions to HDFS-compatible format\n",
    "        output_file = os.path.join(output_dir, f\"{title}_revisions.jsonl\")\n",
    "        save_revisions_to_hdfs(revisions, output_file)\n",
    "        print(f\"Revisions saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
