{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_us_election_titles():\n",
    "    \"\"\"\n",
    "    Returns lists of Wikipedia article titles for US elections in 2016, 2020, and 2024.\n",
    "    \"\"\"\n",
    "    elections_2016 = [\n",
    "        \"2016_United_States_presidential_election\",\n",
    "        \"2016_Democratic_Party_presidential_primaries\",\n",
    "        \"2016_Republican_Party_presidential_primaries\"\n",
    "    ]\n",
    "    \n",
    "    elections_2020 = [\n",
    "        \"2020_United_States_presidential_election\",\n",
    "        \"2020_Democratic_Party_presidential_primaries\",\n",
    "        \"2020_Republican_Party_presidential_primaries\"\n",
    "    ]\n",
    "    elections_2024 = [\n",
    "        \"2024_United_States_presidential_election\",\n",
    "        \"2024_Democratic_Party_presidential_primaries\",\n",
    "        \"2024_Republican_Party_presidential_primaries\"\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"2016\": elections_2016,\n",
    "        \"2020\": elections_2020,\n",
    "        \"2024\": elections_2024\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_articles_in_batches(titles_batch):\n",
    "    \"\"\"\n",
    "    Fetches up to 50 articles in a single API call.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"*\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": \"|\".join(titles_batch)  # Batch up to 50 titles\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"WikiSandbox/ManagingBigData\"}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error fetching articles: {response.status_code}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article(title):\n",
    "    \"\"\"\n",
    "    Fetches a single article by title.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"*\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"WikiSandbox/ManagingBigData\"}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error fetching article {title}: {response.status_code}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_hdfs(data, file_path):\n",
    "    \"\"\"\n",
    "    Saves the data to a file formatted for HDFS ingestion.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for article in data:\n",
    "            json.dump(article, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")  # Write each article as a separate JSON line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_articles(titles, output_file, batch_mode=True):\n",
    "    \"\"\"\n",
    "    Fetches full text for a list of articles and saves them in an HDFS-compatible format.\n",
    "\n",
    "    Args:\n",
    "        titles (list): List of article titles to fetch.\n",
    "        output_file (str): Path to the output file.\n",
    "        batch_mode (bool): Whether to use batch fetching or individual fetching.\n",
    "    \"\"\"\n",
    "    total_saved = 0  # Track the total number of articles saved\n",
    "    batch_size = 50  # Max 50 titles per batch\n",
    "\n",
    "    if batch_mode:\n",
    "        # Process in batches\n",
    "        batches = [titles[i:i + batch_size] for i in range(0, len(titles), batch_size)]\n",
    "        for idx, batch in enumerate(batches, start=1):\n",
    "            print(f\"Fetching batch {idx}/{len(batches)}...\")\n",
    "            try:\n",
    "                data = fetch_articles_in_batches(batch)\n",
    "                processed_data = []\n",
    "                for page_id, page_info in data[\"query\"][\"pages\"].items():\n",
    "                    title = page_info[\"title\"]\n",
    "                    if \"revisions\" in page_info:\n",
    "                        text = page_info[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "                    else:\n",
    "                        text = \"No content available\"\n",
    "                    processed_data.append({\"title\": title, \"text\": text})\n",
    "                save_to_hdfs(processed_data, output_file)\n",
    "                total_saved += len(processed_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch batch {idx}: {e}\")\n",
    "            time.sleep(1)  # Wait 1 second between batches\n",
    "    else:\n",
    "        # Process individually\n",
    "        for idx, title in enumerate(titles, start=1):\n",
    "            print(f\"Fetching article {idx}/{len(titles)}: {title}...\")\n",
    "            try:\n",
    "                data = fetch_article(title)\n",
    "                processed_data = []\n",
    "                for page_id, page_info in data[\"query\"][\"pages\"].items():\n",
    "                    if \"revisions\" in page_info:\n",
    "                        text = page_info[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "                    else:\n",
    "                        text = \"No content available\"\n",
    "                    processed_data.append({\"title\": page_info[\"title\"], \"text\": text})\n",
    "                save_to_hdfs(processed_data, output_file)\n",
    "                total_saved += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch article {title}: {e}\")\n",
    "            time.sleep(1)  # Wait 1 second between requests\n",
    "\n",
    "    print(f\"Total articles saved: {total_saved}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_category_articles(category, include_subcategories=False,write_to_file=False):\n",
    "    def fetch_subcategories(category):\n",
    "        subcategories = []\n",
    "        cmcontinue = \"\"\n",
    "        while True:\n",
    "            url = f\"https://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Category:{category}&cmtype=subcat&cmlimit=max&format=json&cmcontinue={cmcontinue}\"\n",
    "            response = requests.get(url).json()\n",
    "            subcategories.extend([cat[\"title\"] for cat in response[\"query\"][\"categorymembers\"]])\n",
    "            if \"continue\" in response:\n",
    "                cmcontinue = response[\"continue\"][\"cmcontinue\"]\n",
    "            else:\n",
    "                break\n",
    "        return subcategories\n",
    "\n",
    "    def fetch_pages(category):\n",
    "        pages = []\n",
    "        cmcontinue = \"\"\n",
    "        while True:\n",
    "            url = f\"https://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Category:{category}&cmtype=page&cmlimit=max&format=json&cmcontinue={cmcontinue}\"\n",
    "            response = requests.get(url).json()\n",
    "            pages.extend([page[\"title\"] for page in response[\"query\"][\"categorymembers\"]])\n",
    "            if \"continue\" in response:\n",
    "                cmcontinue = response[\"continue\"][\"cmcontinue\"]\n",
    "            else:\n",
    "                break\n",
    "        return pages\n",
    "\n",
    "    articles = fetch_pages(category)\n",
    "    if include_subcategories:\n",
    "        subcategories = fetch_subcategories(category)\n",
    "        for subcategory in subcategories:\n",
    "            articles.extend(fetch_pages(subcategory.replace(\"Category:\", \"\")))\n",
    "\n",
    "    if write_to_file:\n",
    "        with open(f\"{category}_articles.txt\", \"w\") as f:\n",
    "            for article in articles:\n",
    "                f.write(article + \"\\n\")\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching batch 1/21...\n",
      "Fetching batch 2/21...\n",
      "Fetching batch 3/21...\n",
      "Fetching batch 4/21...\n",
      "Fetching batch 5/21...\n",
      "Fetching batch 6/21...\n",
      "Fetching batch 7/21...\n",
      "Fetching batch 8/21...\n",
      "Fetching batch 9/21...\n",
      "Fetching batch 10/21...\n",
      "Fetching batch 11/21...\n",
      "Fetching batch 12/21...\n",
      "Fetching batch 13/21...\n",
      "Fetching batch 14/21...\n",
      "Fetching batch 15/21...\n",
      "Fetching batch 16/21...\n",
      "Fetching batch 17/21...\n",
      "Fetching batch 18/21...\n",
      "Fetching batch 19/21...\n",
      "Fetching batch 20/21...\n",
      "Fetching batch 21/21...\n",
      "Total articles saved: 1024\n"
     ]
    }
   ],
   "source": [
    "# Get US election titles\n",
    "\n",
    "election_titles = fetch_category_articles(\"Elections in the United States\", include_subcategories=True,write_to_file=True)\n",
    "\n",
    " \n",
    "\n",
    "# use the fetch and save articles function to fetch the articles from election_titles; save them into the all_elections_data/all_elections_data.jsonl file\n",
    "\n",
    "fetch_and_save_articles(election_titles, \"all_elections_data/all_elections_data.jsonl\", batch_mode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
