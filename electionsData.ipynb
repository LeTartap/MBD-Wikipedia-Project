{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_us_election_titles():\n",
    "    \"\"\"\n",
    "    Returns lists of Wikipedia article titles for US elections in 2016, 2020, and 2024.\n",
    "    \"\"\"\n",
    "    elections_2016 = [\n",
    "        \"2016_United_States_presidential_election\",\n",
    "        \"2016_Democratic_Party_presidential_primaries\",\n",
    "        \"2016_Republican_Party_presidential_primaries\"\n",
    "    ]\n",
    "    \n",
    "    elections_2020 = [\n",
    "        \"2020_United_States_presidential_election\",\n",
    "        \"2020_Democratic_Party_presidential_primaries\",\n",
    "        \"2020_Republican_Party_presidential_primaries\"\n",
    "    \n",
    "    elections_2024 = [\n",
    "        \"2024_United_States_presidential_election\",\n",
    "        \"2024_Democratic_Party_presidential_primaries\",\n",
    "        \"2024_Republican_Party_presidential_primaries\"\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"2016\": elections_2016,\n",
    "        \"2020\": elections_2020,\n",
    "        \"2024\": elections_2024\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_articles_in_batches(titles_batch):\n",
    "    \"\"\"\n",
    "    Fetches up to 50 articles in a single API call.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"*\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": \"|\".join(titles_batch)  # Batch up to 50 titles\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"WikiSandbox/ManagingBigData\"}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error fetching articles: {response.status_code}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article(title):\n",
    "    \"\"\"\n",
    "    Fetches a single article by title.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"*\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"WikiSandbox/ManagingBigData\"}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error fetching article {title}: {response.status_code}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_hdfs(data, file_path):\n",
    "    \"\"\"\n",
    "    Saves the data to a file formatted for HDFS ingestion.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for article in data:\n",
    "            json.dump(article, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")  # Write each article as a separate JSON line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_articles(titles, output_file, batch_mode=True):\n",
    "    \"\"\"\n",
    "    Fetches full text for a list of articles and saves them in an HDFS-compatible format.\n",
    "\n",
    "    Args:\n",
    "        titles (list): List of article titles to fetch.\n",
    "        output_file (str): Path to the output file.\n",
    "        batch_mode (bool): Whether to use batch fetching or individual fetching.\n",
    "    \"\"\"\n",
    "    total_saved = 0  # Track the total number of articles saved\n",
    "    batch_size = 50  # Max 50 titles per batch\n",
    "\n",
    "    if batch_mode:\n",
    "        # Process in batches\n",
    "        batches = [titles[i:i + batch_size] for i in range(0, len(titles), batch_size)]\n",
    "        for idx, batch in enumerate(batches, start=1):\n",
    "            print(f\"Fetching batch {idx}/{len(batches)}...\")\n",
    "            try:\n",
    "                data = fetch_articles_in_batches(batch)\n",
    "                processed_data = []\n",
    "                for page_id, page_info in data[\"query\"][\"pages\"].items():\n",
    "                    title = page_info[\"title\"]\n",
    "                    if \"revisions\" in page_info:\n",
    "                        text = page_info[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "                    else:\n",
    "                        text = \"No content available\"\n",
    "                    processed_data.append({\"title\": title, \"text\": text})\n",
    "                save_to_hdfs(processed_data, output_file)\n",
    "                total_saved += len(processed_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch batch {idx}: {e}\")\n",
    "            time.sleep(1)  # Wait 1 second between batches\n",
    "    else:\n",
    "        # Process individually\n",
    "        for idx, title in enumerate(titles, start=1):\n",
    "            print(f\"Fetching article {idx}/{len(titles)}: {title}...\")\n",
    "            try:\n",
    "                data = fetch_article(title)\n",
    "                processed_data = []\n",
    "                for page_id, page_info in data[\"query\"][\"pages\"].items():\n",
    "                    if \"revisions\" in page_info:\n",
    "                        text = page_info[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "                    else:\n",
    "                        text = \"No content available\"\n",
    "                    processed_data.append({\"title\": page_info[\"title\"], \"text\": text})\n",
    "                save_to_hdfs(processed_data, output_file)\n",
    "                total_saved += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch article {title}: {e}\")\n",
    "            time.sleep(1)  # Wait 1 second between requests\n",
    "\n",
    "    print(f\"Total articles saved: {total_saved}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for US Elections 2016...\n",
      "Fetching article 1/3: 2016_United_States_presidential_election...\n",
      "Fetching article 2/3: 2016_Democratic_Party_presidential_primaries...\n",
      "Fetching article 3/3: 2016_Republican_Party_presidential_primaries...\n",
      "Total articles saved: 3\n",
      "Fetching articles for US Elections 2020...\n",
      "Fetching article 1/1: 2020_United_States_presidential_election...\n",
      "Total articles saved: 1\n",
      "Fetching articles for US Elections 2024...\n",
      "Fetching article 1/1: 2024_United_States_presidential_election...\n",
      "Total articles saved: 1\n"
     ]
    }
   ],
   "source": [
    "# Get US election titles\n",
    "election_titles = get_us_election_titles()\n",
    "batch_mode = False  # Set to False to disable batch fetching\n",
    "\n",
    "# Process each election year\n",
    "for year, titles in election_titles.items():\n",
    "    output_file = f\"hdfs_data/us_elections_{year}.jsonl\"\n",
    "    print(f\"Fetching articles for US Elections {year}...\")\n",
    "    fetch_and_save_articles(titles, output_file, batch_mode=batch_mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
